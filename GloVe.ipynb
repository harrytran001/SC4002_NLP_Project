{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.0.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.25.1)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.22.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: gensim in /opt/anaconda3/lib/python3.12/site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/anaconda3/lib/python3.12/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /opt/anaconda3/lib/python3.12/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n",
    "!pip install gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Rotten Tomatoes dataset\n",
    "dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_dataset = dataset['train']\n",
    "validation_dataset = dataset['validation']\n",
    "test_dataset = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 8530\n",
       "})"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 1066\n",
       "})"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Classification task on movie reviews:\n",
    "\n",
    "GloVe embeddings may be a better fit because they capture global semantic relationships well, which are important for distinguishing sentiment.\n",
    "\n",
    "Reviews often involve subtle nuances in language (e.g., \"great\" vs. \"not great\"), and GloVe's global understanding can help capture these nuances better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load GloVe embeddings\n",
    "def load_glove_embeddings(file_path):\n",
    "    embedding_dict = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_dict[word] = vector\n",
    "    return embedding_dict\n",
    "\n",
    "glove_embeddings = load_glove_embeddings(\"glove.6B/glove.6B.300d.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download from https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "From there, there is 50d, 100d, 200d, 300d\n",
    "\n",
    "300d would be better but require more computational resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 16513\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Tokenize text\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Build vocabulary from the training data\n",
    "word_counter = Counter()\n",
    "for sample in train_dataset:\n",
    "    word_counter.update(tokenize(sample['text']))\n",
    "\n",
    "# Create word-to-index mapping\n",
    "vocab = {word: idx + 1 for idx, (word, _) in enumerate(word_counter.items())}\n",
    "vocab_size = len(vocab) + 1  # +1 for padding index 0\n",
    "\n",
    "print(f\"Vocabulary Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first version doesn't explicitly handle padding, which may lead to issues during training if padding is required. It assumes a vocabulary without additional padding and might be better suited for fixed-length data.\n",
    "\n",
    "# embedding_dim = 300 # you need to change this if using other dimensions\n",
    "# embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "# oov_words = []\n",
    "\n",
    "# for word, idx in vocab.items():\n",
    "#     if word in glove_embeddings:\n",
    "#         embedding_matrix[idx] = glove_embeddings[word]\n",
    "#     else:\n",
    "#         oov_words.append(word)\n",
    "#         embedding_matrix[idx] = np.random.uniform(-0.05, 0.05, embedding_dim)\n",
    "\n",
    "# print(f\"Number of OOV words: {len(oov_words)}\")\n",
    "\n",
    "# 300d - Test Accuracy: 0.5056\n",
    "# 200d - Test Accuracy: 0.5066\n",
    "# 100d - Test Accuracy: 0.5075\n",
    "# 50d - Test Accuracy: 0.5084\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of OOV words: 591\n"
     ]
    }
   ],
   "source": [
    "# Initialize embedding matrix\n",
    "embedding_dim = 300\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))  # Row 0 is for padding (all zeros)\n",
    "\n",
    "# Compute the average of all known word embeddings for OOV initialization\n",
    "known_embeddings = np.array(list(glove_embeddings.values()))\n",
    "average_embedding = np.mean(known_embeddings, axis=0)\n",
    "\n",
    "oov_words = []\n",
    "\n",
    "# Fill the embedding matrix\n",
    "for word, idx in vocab.items():\n",
    "    if word in glove_embeddings:\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "    else:\n",
    "        oov_words.append(word)\n",
    "        # Assign the average of all known embeddings for OOV words\n",
    "        embedding_matrix[idx] = average_embedding\n",
    "\n",
    "print(f\"Number of OOV words: {len(oov_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existence of the OOV words is one of the well-known limitations of Word2vec (or Glove).\n",
    "Without using any transformer-based language models (e.g., BERT, GPT, T5), what do you\n",
    "think is the best strategy to mitigate such limitation? Implement your solution in your source\n",
    "code. Show the corresponding code snippet.\n",
    "\n",
    "To mitigate the issue of OOV words without using transformer-based models, the best strategy is to randomly initialize the embeddings for OOV words and then allow these embeddings to be updated during training. This way, the model learns suitable representations for these words during training. The above code snippet already implements this strategy by assigning a small random value to OOV words and allowing the model to learn embeddings.\n",
    "\n",
    "if word in glove_embeddings:\n",
    "    embedding_matrix[idx] = glove_embeddings[word]\n",
    "else:\n",
    "    oov_words.append(word)\n",
    "    embedding_matrix[idx] = np.random.uniform(-0.05, 0.05, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.45.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Assuming vocab and embedding_matrix are already prepared\n",
    "\n",
    "# Tokenizing the dataset\n",
    "def tokenize_sentence(sentence, vocab):\n",
    "    tokens = tokenize(sentence)\n",
    "    return [vocab.get(token, 0) for token in tokens]  # Replace OOV words with index 0\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab):\n",
    "        self.sentences = [tokenize_sentence(sample['text'], vocab) for sample in dataset]\n",
    "        self.labels = [sample['label'] for sample in dataset]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Creating the datasets and data loaders\n",
    "train_data = SentimentDataset(train_dataset, vocab)\n",
    "val_data = SentimentDataset(validation_dataset, vocab)\n",
    "test_data = SentimentDataset(test_dataset, vocab)\n",
    "\n",
    "# Padding function for data loader\n",
    "def collate_fn(batch):\n",
    "    sentences, labels = zip(*batch)\n",
    "    sentences_padded = pad_sequence(sentences, batch_first=True, padding_value=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    return sentences_padded, labels\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Vocabulary Size (including padding): 15922\n",
      "Adjusted Embedding Matrix Shape: (15922, 300)\n"
     ]
    }
   ],
   "source": [
    "# Adjust vocabulary to match the embedding matrix size\n",
    "# Create a new vocabulary using only the words present in the GloVe embeddings\n",
    "filtered_vocab = {word: idx + 1 for idx, word in enumerate(glove_embeddings.keys()) if word in word_counter}\n",
    "padding_idx = 0\n",
    "vocab_size = len(filtered_vocab) + 1  # Adding one for the padding index\n",
    "\n",
    "print(f\"Adjusted Vocabulary Size (including padding): {vocab_size}\")\n",
    "\n",
    "# Adjust embedding matrix to include padding\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))  # Initialize with zeros for padding\n",
    "\n",
    "# Fill embedding matrix with GloVe vectors\n",
    "for word, idx in filtered_vocab.items():\n",
    "    if idx < vocab_size:  # Ensure the index does not go out of bounds\n",
    "        embedding_matrix[idx] = glove_embeddings[word]\n",
    "\n",
    "print(f\"Adjusted Embedding Matrix Shape: {embedding_matrix.shape}\")\n",
    "\n",
    "# Ensure the embedding matrix matches the vocab size\n",
    "assert embedding_matrix.shape[0] == vocab_size, \"Embedding matrix rows do not match vocab size\"\n",
    "\n",
    "# Define the RNN model\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.embedding.weight.data.copy_(torch.tensor(embedding_matrix, dtype=torch.float))\n",
    "        self.embedding.weight.requires_grad = False  # Freeze embeddings\n",
    "\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        _, hidden = self.rnn(embedded)\n",
    "        output = self.fc(hidden.squeeze(0))\n",
    "        return output\n",
    "\n",
    "# Model parameters\n",
    "hidden_dim = 128\n",
    "output_dim = 2  # Sentiment (positive or negative)\n",
    "\n",
    "# Initialize the model\n",
    "model = RNNModel(vocab_size, embedding_dim, hidden_dim, output_dim, embedding_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Tokenizing the dataset and ensuring valid index range\n",
    "def tokenize_sentence(sentence, vocab, vocab_size):\n",
    "    tokens = tokenize(sentence)\n",
    "    # Convert tokens to indices and clamp them to be within the valid range [0, vocab_size - 1]\n",
    "    return [min(vocab.get(token, 0), vocab_size - 1) for token in tokens]  # Use index 0 for OOV words\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, dataset, vocab, vocab_size):\n",
    "        self.sentences = [tokenize_sentence(sample['text'], vocab, vocab_size) for sample in dataset]\n",
    "        self.labels = [sample['label'] for sample in dataset]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.sentences[idx]), torch.tensor(self.labels[idx])\n",
    "\n",
    "# Create datasets and dataloaders with clamped indices\n",
    "train_data = SentimentDataset(train_dataset, filtered_vocab, vocab_size)\n",
    "val_data = SentimentDataset(validation_dataset, filtered_vocab, vocab_size)\n",
    "test_data = SentimentDataset(test_dataset, filtered_vocab, vocab_size)\n",
    "\n",
    "# Dataloader setup\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 92.9934, Train Accuracy: 0.4951, Validation Accuracy: 0.5009\n",
      "Epoch [2/20], Train Loss: 92.8584, Train Accuracy: 0.4931, Validation Accuracy: 0.4944\n",
      "Epoch [3/20], Train Loss: 92.7839, Train Accuracy: 0.5018, Validation Accuracy: 0.5038\n",
      "Epoch [4/20], Train Loss: 92.6633, Train Accuracy: 0.5012, Validation Accuracy: 0.4962\n",
      "Epoch [5/20], Train Loss: 92.6632, Train Accuracy: 0.4980, Validation Accuracy: 0.4887\n",
      "Epoch [6/20], Train Loss: 92.6636, Train Accuracy: 0.5062, Validation Accuracy: 0.4962\n",
      "Early stopping after epoch 6\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Dataloader setup (train and validation)\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Training loop\n",
    "best_val_acc = 0\n",
    "patience = 3  # Stop training if validation accuracy does not improve for 3 consecutive epochs\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for sentences, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero out gradients\n",
    "        output = model(sentences)\n",
    "        \n",
    "        # Calculate loss and backpropagate\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Calculate accuracy during training\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for sentences, labels in val_loader:\n",
    "            output = model(sentences)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "\n",
    "    val_acc = correct_val / total_val\n",
    "\n",
    "    # Output training and validation results for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping if validation accuracy is not improving\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_improve = 0  # Reset the counter if the validation accuracy improves\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epochs_no_improve == patience:\n",
    "        print(f\"Early stopping after epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "### Summary of Best Model Configuration:\n",
    "# - Learning Rate: 0.001\n",
    "# - Batch Size: 64\n",
    "# - Optimizer: Adam\n",
    "# - Epochs: Stop early if no improvement for 3 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5094\n"
     ]
    }
   ],
   "source": [
    "# Test Dataloader setup\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Evaluation on Test Set\n",
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for sentences, labels in test_loader:\n",
    "        output = model(sentences)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = correct_test / total_test\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300d - Test Accuracy: 0.5103\n",
    "200d - Test Accuracy: 0.5019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
